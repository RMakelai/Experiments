import math

def reward_function(params):

    #####
    # Read input parameters
    #####

    track_width = params['track_width']
    distance_from_center = params['distance_from_center']
    all_wheels_on_track = params['all_wheels_on_track']
    speed = params['speed']
    waypoints = params['waypoints']
    closest_waypoints = params['closest_waypoints']
    heading = params['heading']
    steps = params["steps"]
    progress = params["progress"]
    steering_angle = params['steering_angle']
    is_offtrack = params["is_offtrack"]
    is_reversed = params["is_reversed"]

    objects_location = params['objects_location']
    agent_x = params['x']
    agent_y = params['y']
#    _, next_object_index = params['closest_objects']
    next_object_index,_ = params['closest_objects']
    objects_left_of_center = params['objects_left_of_center']
    is_left_of_center = params['is_left_of_center']
    

    # Give a very low reward by default
    reward = 0.01

    # Sefl-motivator to run track as fast as possible and with as few steps as possible
    if not (is_offtrack or is_reversed) and steps > 0:
        reward += ((progress / steps) * 100) + speed*speed
    else:
        reward += 0.01

    ### Time-like reward:
    if progress == 100:
        reward += 11000 / steps
    
    if (is_offtrack or is_reversed):
        reward += -1500
    
    if progress >= 0 and progress < 25 and speed > 3 and steering_angle == 0 and not (is_offtrack or is_reversed):
        reward += 500 + speed*speed
   
    if progress >= 50 and progress < 70 and speed > 3 and steering_angle == 0 and not (is_offtrack or is_reversed):
        reward += 500 + speed*speed
    
    #####	
    # Rewarding the agent for staying on track and following center line
    #####
    #if all_wheels_on_track:
    #if not (is_offtrack or is_reversed):
    #    reward += 15 / (distance_from_center * distance_from_center +1)
    #else: 
    #    reward += 0.01

    #if all_wheels_on_track:
    #    reward += 1

    ####
    # Rewarding the agent for speed
    ####

    #if not (is_offtrack or is_reversed):
    #    reward += speed * speed / 1.5
    #else:
    #    reward += 0.01


    #####
    # Reward for going fast on straight
    #####
    
    #if abs(steering_angle) < 0.5 and speed > 2.0:
    #    reward += 4.5 * speed
        

    #####	
    # Rewarding the agent for making progress
    #####
    
    #if all_wheels_on_track and steps > 0:
    #if not (is_offtrack or is_reversed) and steps > 0:
    #    reward += 175 * (progress / steps)
    #else:
    #    reward += 0.01
    

    #####
    # Reward for quick start
    #####
    
    #if steps < 11 and speed > 3.0:
    #    reward += 10
    #if (progress >= 0 and progress < 2.1 and abs(steering_angle) < 3 and speed > 2.2) or (progress >= 75 and progress <= 100 and abs(steering_angle) < 3 and speed > 2.2):
    #    reward += speed * speed 

    ####
    # Reward for slow speed when steering angle high and then going fast straight ahead
    ####

    #if not (is_offtrack or is_reversed):
    #    reward += 5 * (0.5 + (1 * math.sin(0.5 * speed / 4 + 0.75 * abs(steering_angle) / 30 + 0.9) - 1) / 0.5)
  

    ####
    # Penalty if off-track
    ####

    #if is_offtrack or is_reversed:
    #    reward -= 900

    #if not all_wheels_on_track:
    #    reward -= 10

    #####	
    # Reward for completing lap
    #####
    
    #if progress == 25:
    #    reward += 25

    #if progress == 50:
    #    reward += 50

    #if progress == 75:
    #    reward += 75
    
    #if progress == 100:
    #    reward += 50

    #    print(steps)
       
       
    # Return reward value
    return float(reward)